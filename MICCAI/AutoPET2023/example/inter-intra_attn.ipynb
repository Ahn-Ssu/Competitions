{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self) -> None: #  *args, **kwargs\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        embed_dim = embed_dim\n",
    "        num_heads = num_heads\n",
    "        dropout = dropout\n",
    "        head_dim = embed_dim // num_heads\n",
    "        assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128x128x128=2097152\n",
      "torch.Size([1, 2097152, 64]) torch.Size([1, 2097152, 64])\n"
     ]
    }
   ],
   "source": [
    "h_ct = torch.rand((1, 64, 128, 128, 128))\n",
    "h_pet = torch.rand((1, 64, 128, 128, 128))\n",
    "print(f'128x128x128={128*128*128}')\n",
    "\n",
    "attn = nn.MultiheadAttention(embed_dim=64, num_heads=4)\n",
    "transformer = nn.TransformerDecoderLayer(d_model=64, nhead=4, dim_feedforward=64*4, activation=F.selu)\n",
    "\n",
    "### Attention test\n",
    "bz, d, *size = h_pet.shape\n",
    "h_pet = h_pet.view(bz, d, -1).contiguous().transpose(1,2)\n",
    "h_ct = h_ct.view(bz, d, -1).contiguous().transpose(1,2)\n",
    "\n",
    "ct_inter, w_ = attn(h_pet, h_ct, h_ct)\n",
    "pet_inter, w2 = attn(h_ct, h_pet, h_pet)\n",
    "\n",
    "print(ct_inter.shape, pet_inter.shape)\n",
    "\n",
    "### Transformer\n",
    "\n",
    "ct_inter = transformer(tgt=h_pet, memory=h_pet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntraInter_Attention(nn.Module):\n",
    "    def __init__(self, d_model, n_head=4) -> None:\n",
    "        super(IntraInter_Attention, self).__init__()\n",
    "        # transformer 사용 vs Multihead Attn 사용 비교 필요 \n",
    "        assert d_model % n_head == 0 \n",
    "\n",
    "        self.PET_inter = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_head, dim_feedforward=d_model*4, activation=F.selu)\n",
    "        self.CT_inter = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_head, dim_feedforward=d_model*4, activation=F.selu)\n",
    "\n",
    "        self.PET_intra = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_head, dim_feedforward=d_model*4, activation=F.selu)\n",
    "        self.CT_intra = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_head, dim_feedforward=d_model*4, activation=F.selu)\n",
    "\n",
    "    def forward(self, h_ct, h_pet):\n",
    "        bz, d, *size = h_pet.shape\n",
    "\n",
    "        h_pet = h_pet.view(bz, d, -1).contiguous().transpose(1,2)\n",
    "        h_ct = h_ct.view(bz, d, -1).contiguous().transpose(1,2)\n",
    "\n",
    "        pet_inter = self.PET_inter(h_pet, h_ct)\n",
    "        ct_inter  = self.CT_inter(h_ct, h_pet)\n",
    "\n",
    "        pet_intra = self.PET_intra(h_pet, h_pet)\n",
    "        ct_intra  = self.CT_intra(h_ct, h_ct)\n",
    "\n",
    "        out = torch.concat([pet_inter, ct_inter, pet_intra, ct_intra], dim=2)\n",
    "        out = out.transpose(1, 2).view(bz, d, size[0], size[1], size[2])\n",
    "\n",
    "        return out # return shape: bz, d*4, *size_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoPET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
