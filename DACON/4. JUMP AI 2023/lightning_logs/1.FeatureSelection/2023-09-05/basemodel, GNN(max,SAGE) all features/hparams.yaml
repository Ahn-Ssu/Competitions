args: !!python/object/new:easydict.EasyDict
  dictitems:
    FF_hidden_dim: 2048
    atomic_feature_list: &id001
    - SH
    - DMPNN
    - BPS
    batch_size: 16
    epoch: 100
    gnn_hidden_dims: &id002
    - 64
    - 128
    - 256
    - 512
    init_lr: 1.0e-05
    lr_dec_rate: 0.01
    mol_feature_list: &id003
    - new
    - rdkit
    num_atom_f: 167
    num_fp_f: 300
    num_mol_f: 219
    out_dim: 2
    projection_dim: 1479
    seed: 41
    server: mk3
    weight_decay: 0.1
    z_model_arch: "Molp(\n  (gnn_encoder): GIN(\n    (conv1): SAGEConv(167, 64, aggr=add)\n\
      \    (conv2): SAGEConv(64, 128, aggr=add)\n    (conv3): SAGEConv(128, 256, aggr=add)\n\
      \    (conv4): SAGEConv(256, 512, aggr=add)\n    (norm1): InstanceNorm(64)\n\
      \    (norm2): InstanceNorm(128)\n    (norm3): InstanceNorm(256)\n    (norm4):\
      \ InstanceNorm(512)\n    (act): SELU()\n  )\n  (fc1): Linear(in_features=1479,\
      \ out_features=2048, bias=True)\n  (fc2): Linear(in_features=2048, out_features=2048,\
      \ bias=True)\n  (fc3): Linear(in_features=2048, out_features=2048, bias=True)\n\
      \  (fc_out): Linear(in_features=2048, out_features=2, bias=True)\n  (ln1): LayerNorm((2048,),\
      \ eps=1e-05, elementwise_affine=True)\n  (ln2): LayerNorm((2048,), eps=1e-05,\
      \ elementwise_affine=True)\n  (ln3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n\
      \  (activation): SELU()\n  (dropout): Dropout(p=0.7, inplace=False)\n)"
  state:
    FF_hidden_dim: 2048
    atomic_feature_list: *id001
    batch_size: 16
    epoch: 100
    gnn_hidden_dims: *id002
    init_lr: 1.0e-05
    lr_dec_rate: 0.01
    mol_feature_list: *id003
    num_atom_f: 167
    num_fp_f: 300
    num_mol_f: 219
    out_dim: 2
    projection_dim: 1479
    seed: 41
    server: mk3
    weight_decay: 0.1
    z_model_arch: "Molp(\n  (gnn_encoder): GIN(\n    (conv1): SAGEConv(167, 64, aggr=add)\n\
      \    (conv2): SAGEConv(64, 128, aggr=add)\n    (conv3): SAGEConv(128, 256, aggr=add)\n\
      \    (conv4): SAGEConv(256, 512, aggr=add)\n    (norm1): InstanceNorm(64)\n\
      \    (norm2): InstanceNorm(128)\n    (norm3): InstanceNorm(256)\n    (norm4):\
      \ InstanceNorm(512)\n    (act): SELU()\n  )\n  (fc1): Linear(in_features=1479,\
      \ out_features=2048, bias=True)\n  (fc2): Linear(in_features=2048, out_features=2048,\
      \ bias=True)\n  (fc3): Linear(in_features=2048, out_features=2048, bias=True)\n\
      \  (fc_out): Linear(in_features=2048, out_features=2, bias=True)\n  (ln1): LayerNorm((2048,),\
      \ eps=1e-05, elementwise_affine=True)\n  (ln2): LayerNorm((2048,), eps=1e-05,\
      \ elementwise_affine=True)\n  (ln3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n\
      \  (activation): SELU()\n  (dropout): Dropout(p=0.7, inplace=False)\n)"
