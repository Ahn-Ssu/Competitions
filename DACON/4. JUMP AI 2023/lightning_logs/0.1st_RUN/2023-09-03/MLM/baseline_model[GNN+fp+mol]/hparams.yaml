args: !!python/object/new:easydict.EasyDict
  dictitems:
    FF_hidden_dim: 2048
    MLM: true
    batch_size: 64
    epoch: 1000
    gnn_hidden_dims: &id001
    - 64
    - 128
    - 256
    - 512
    init_lr: 0.0001
    lr_dec_rate: 0.01
    num_atom_f: 34
    num_fp_f: 5235
    num_mol_f: 36
    projection_dim: 6231
    seed: 41
    server: mk3
    train_label: MLM
    weight_decay: 0.05
    z_model_arch: "Molp(\n  (gnn_encoder): GIN(\n    (conv1): SAGEConv(34, 64, aggr=add)\n\
      \    (conv2): SAGEConv(64, 128, aggr=add)\n    (conv3): SAGEConv(128, 256, aggr=add)\n\
      \    (conv4): SAGEConv(256, 512, aggr=add)\n    (norm1): InstanceNorm(64)\n\
      \    (norm2): InstanceNorm(128)\n    (norm3): InstanceNorm(256)\n    (norm4):\
      \ InstanceNorm(512)\n    (act): SELU()\n  )\n  (fc1): Linear(in_features=6231,\
      \ out_features=2048, bias=True)\n  (fc2): Linear(in_features=2048, out_features=2048,\
      \ bias=True)\n  (fc3): Linear(in_features=2048, out_features=2048, bias=True)\n\
      \  (fc_out): Linear(in_features=2048, out_features=1, bias=True)\n  (ln1): LayerNorm((2048,),\
      \ eps=1e-05, elementwise_affine=True)\n  (ln2): LayerNorm((2048,), eps=1e-05,\
      \ elementwise_affine=True)\n  (ln3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n\
      \  (activation): LeakyReLU(negative_slope=0.01)\n  (dropout): Dropout(p=0.7,\
      \ inplace=False)\n)"
  state:
    FF_hidden_dim: 2048
    MLM: true
    batch_size: 64
    epoch: 1000
    gnn_hidden_dims: *id001
    init_lr: 0.0001
    lr_dec_rate: 0.01
    num_atom_f: 34
    num_fp_f: 5235
    num_mol_f: 36
    projection_dim: 6231
    seed: 41
    server: mk3
    train_label: MLM
    weight_decay: 0.05
    z_model_arch: "Molp(\n  (gnn_encoder): GIN(\n    (conv1): SAGEConv(34, 64, aggr=add)\n\
      \    (conv2): SAGEConv(64, 128, aggr=add)\n    (conv3): SAGEConv(128, 256, aggr=add)\n\
      \    (conv4): SAGEConv(256, 512, aggr=add)\n    (norm1): InstanceNorm(64)\n\
      \    (norm2): InstanceNorm(128)\n    (norm3): InstanceNorm(256)\n    (norm4):\
      \ InstanceNorm(512)\n    (act): SELU()\n  )\n  (fc1): Linear(in_features=6231,\
      \ out_features=2048, bias=True)\n  (fc2): Linear(in_features=2048, out_features=2048,\
      \ bias=True)\n  (fc3): Linear(in_features=2048, out_features=2048, bias=True)\n\
      \  (fc_out): Linear(in_features=2048, out_features=1, bias=True)\n  (ln1): LayerNorm((2048,),\
      \ eps=1e-05, elementwise_affine=True)\n  (ln2): LayerNorm((2048,), eps=1e-05,\
      \ elementwise_affine=True)\n  (ln3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n\
      \  (activation): LeakyReLU(negative_slope=0.01)\n  (dropout): Dropout(p=0.7,\
      \ inplace=False)\n)"
