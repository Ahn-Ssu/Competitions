{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2023-09-06 07:16:47--  https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/vocab.txt\n",
      "Resolving deepchemdata.s3-us-west-1.amazonaws.com (deepchemdata.s3-us-west-1.amazonaws.com)... 3.5.160.160, 52.219.117.146, 3.5.162.13, ...\n",
      "Connecting to deepchemdata.s3-us-west-1.amazonaws.com (deepchemdata.s3-us-west-1.amazonaws.com)|3.5.160.160|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3524 (3.4K) [text/plain]\n",
      "Saving to: ‘vocab.txt’\n",
      "\n",
      "vocab.txt           100%[===================>]   3.44K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-09-06 07:16:48 (12.9 MB/s) - ‘vocab.txt’ saved [3524/3524]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.feat.smiles_tokenizer import SmilesTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForMaskedLM, AutoModel, AutoTokenizer, pipeline, RobertaModel, RobertaTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "model = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\", num_labels=2, problem_type=\"multi_label_classification\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(600, 384, padding_idx=1)\n",
       "    (position_embeddings): Embedding(515, 384, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.144, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-2): 3 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.109, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.144, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=464, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=464, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.144, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmilesTokenizer(name_or_path='', vocab_size=591, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = '/root/Competitions/DACON/4. JUMP AI 2023/example/vocab.txt'\n",
    "\n",
    "# Pt로 데려온것이랑 차이 \n",
    "# DC tokenizer -> input_ids, token_type_ids, attention_mask 를 제공\n",
    "# Hugging Face -> input_ids, x , attention_mask\n",
    "## tokenizer.encoder(smiles) -> input_ids를 리턴 \n",
    "deep_chem_tokenizer = SmilesTokenizer(vocab_file=vocab, max_len=model.config.max_position_embeddings)\n",
    "deep_chem_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[12, 16, 16, 17, 16, 18, 23, 15, 20, 15, 15, 25, 15, 17, 23, 21, 16, 16,\n",
      "         23, 17, 16, 15, 26, 15, 15, 15, 42, 26, 18, 16, 17, 16, 16, 19, 18, 16,\n",
      "         21, 18, 25, 20, 13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "input_ids torch.Size([1, 41])\n",
      "attention_mask torch.Size([1, 41])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0923, -0.0815]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles1 = 'CC(C)Nc1ccnc(N2CCN(Cc3cccs3)C(CCO)C2)n1'\n",
    "smiles2 = 'C1=Cn2nc(/C=C/c3cccs3)nc2-c2ccccc2O1'\n",
    "smiles3 = 'CCN1CCN(C(=O)c2cc3c(=O)n4cc(C)ccc4nc3n2C)CC1'\n",
    "\n",
    "batch = [smiles1]\n",
    "inputs = tokenizer.encode_plus(smiles1, padding=True, truncation=True, return_tensors='pt', add_special_tokens=True, max_length=model.config.max_position_embeddings)\n",
    "print(inputs)\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "print('input_ids', input_ids.shape)\n",
    "print('attention_mask', attention_mask.shape)\n",
    "ret = model(input_ids)\n",
    "print(ret)\n",
    "attention = ret[-1]\n",
    "input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list)# head_view(attention, tokens)\n",
    "\n",
    "ret.logits.shape\n",
    "# print(ret.pooler_output.shape, ret.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 99.3kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 2.10MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 1.10MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 1.12MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102], [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], [101, 1327, 1164, 5450, 23434, 136, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.molnet import load_bbbp, load_clearance, load_clintox, load_delaney, load_hiv, load_qm7, load_tox21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "tasks, (train_df, valid_df, test_df), transformers = load_clearance()\n",
    "\n",
    "def proc(data):\n",
    "    y = data.y\n",
    "    ids = data.ids \n",
    "    df = np.concatenate([ids.reshape(-1, 1), y], axis=1)\n",
    "    df = pd.DataFrame(df, columns=['text', 'labels'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CS(=O)(=O)c1ccc(cc1)[C@@H](O)[C@@H](CF)NC(=O)C...</td>\n",
       "      <td>4.251063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(C)CCOc1ccc(N)cc1</td>\n",
       "      <td>5.01728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COc1cc(cc(OC)c1O)C(=O)OCCCCNC(=N)N</td>\n",
       "      <td>4.934762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C=CC(=O)Nc1ccccc1</td>\n",
       "      <td>3.507358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C[C@@H](C(=O)NS(=O)(=O)C)c1ccc(OS(=O)(=O)C(F)(...</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>C#Cc1cccc(Nc2ncnc3cc4OCCOCCOCCOc4cc23)c1</td>\n",
       "      <td>3.130263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>O=C(N1CCOCC1)N2CCn3cc(C4=C(C(=O)NC4=O)c5cnc6cc...</td>\n",
       "      <td>3.663562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>CCOc1nc2cccc(C(=O)O)c2n1Cc3ccc(cc3)c4ccccc4c5n...</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>NC(=O)NC(=O)C(Nc1ccc2CCCc2c1)c3ccccc3</td>\n",
       "      <td>4.024458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>CO[C@@H]1CC[C@@]2(CC1)Cc3ccc(cc3C24N=C(C)C(=N4...</td>\n",
       "      <td>2.609334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>669 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text    labels\n",
       "0    CS(=O)(=O)c1ccc(cc1)[C@@H](O)[C@@H](CF)NC(=O)C...  4.251063\n",
       "1                                  CC(C)CCOc1ccc(N)cc1   5.01728\n",
       "2                   COc1cc(cc(OC)c1O)C(=O)OCCCCNC(=N)N  4.934762\n",
       "3                                    C=CC(=O)Nc1ccccc1  3.507358\n",
       "4    C[C@@H](C(=O)NS(=O)(=O)C)c1ccc(OS(=O)(=O)C(F)(...  1.386294\n",
       "..                                                 ...       ...\n",
       "664           C#Cc1cccc(Nc2ncnc3cc4OCCOCCOCCOc4cc23)c1  3.130263\n",
       "665  O=C(N1CCOCC1)N2CCn3cc(C4=C(C(=O)NC4=O)c5cnc6cc...  3.663562\n",
       "666  CCOc1nc2cccc(C(=O)O)c2n1Cc3ccc(cc3)c4ccccc4c5n...  1.386294\n",
       "667              NC(=O)NC(=O)C(Nc1ccc2CCCc2c1)c3ccccc3  4.024458\n",
       "668  CO[C@@H]1CC[C@@]2(CC1)Cc3ccc(cc3C24N=C(C)C(=N4...  2.609334\n",
       "\n",
       "[669 rows x 2 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = proc(train_df)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O[C@@H](CNCCOc1cccc(CNCCc2ccccc2F)c1)c3ccc(O)c...</td>\n",
       "      <td>3.178054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clc1ccc(CN2CCNCC2)cc1C(=O)NCC34CC5CC(CC(C5)C3)C4</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COCCN(C)c1ccc(Nc2ncc3cc(ccc3n2)c4ccncc4)cc1</td>\n",
       "      <td>3.73146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C[C@@H](N(CC1CCS(=O)(=O)CC1)C(=O)Cc2ccc(c(F)c2...</td>\n",
       "      <td>2.089392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC(C)N1CCN(Cc2oc(nc2)c3cc(cc4[nH]ncc34)c5cccc6...</td>\n",
       "      <td>2.781301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>O[C@@H](CNCCSCCCOCCc1ccccc1)c2ccc(O)c3NC(=O)Sc23</td>\n",
       "      <td>5.01728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Cc1ccccc1c2c(C(=O)O)n(CCCOc3cccc4ccccc34)c5ccc...</td>\n",
       "      <td>4.820523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Clc1ccc2ncc(c3cccc(NC4CNC4)n3)n2c1</td>\n",
       "      <td>2.998229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>OCc1cc(ccc1O)C(O)CNCCCCCCOCCCCc2ccccc2</td>\n",
       "      <td>4.454347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Cc1cc(C)nc(SCC(=O)N)n1</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text    labels\n",
       "0   O[C@@H](CNCCOc1cccc(CNCCc2ccccc2F)c1)c3ccc(O)c...  3.178054\n",
       "1    Clc1ccc(CN2CCNCC2)cc1C(=O)NCC34CC5CC(CC(C5)C3)C4  1.609438\n",
       "2         COCCN(C)c1ccc(Nc2ncc3cc(ccc3n2)c4ccncc4)cc1   3.73146\n",
       "3   C[C@@H](N(CC1CCS(=O)(=O)CC1)C(=O)Cc2ccc(c(F)c2...  2.089392\n",
       "4   CC(C)N1CCN(Cc2oc(nc2)c3cc(cc4[nH]ncc34)c5cccc6...  2.781301\n",
       "..                                                ...       ...\n",
       "79   O[C@@H](CNCCSCCCOCCc1ccccc1)c2ccc(O)c3NC(=O)Sc23   5.01728\n",
       "80  Cc1ccccc1c2c(C(=O)O)n(CCCOc3cccc4ccccc34)c5ccc...  4.820523\n",
       "81                 Clc1ccc2ncc(c3cccc(NC4CNC4)n3)n2c1  2.998229\n",
       "82             OCc1cc(ccc1O)C(O)CNCCCCCCOCCCCc2ccccc2  4.454347\n",
       "83                             Cc1cc(C)nc(SCC(=O)N)n1  1.386294\n",
       "\n",
       "[84 rows x 2 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df = proc(valid_df)\n",
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module.train() got an unexpected keyword argument 'train_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mtrain(train_dataset\u001b[39m=\u001b[39;49mtrain_df, val_dataset\u001b[39m=\u001b[39;49mvalid_df)\n",
      "\u001b[0;31mTypeError\u001b[0m: Module.train() got an unexpected keyword argument 'train_dataset'"
     ]
    }
   ],
   "source": [
    "from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
